{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T19:24:31.763531Z",
     "iopub.status.idle": "2024-05-08T19:24:31.764322Z",
     "shell.execute_reply": "2024-05-08T19:24:31.764094Z",
     "shell.execute_reply.started": "2024-05-08T19:24:31.764064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T19:36:04.368383Z",
     "iopub.status.busy": "2024-05-08T19:36:04.368001Z",
     "iopub.status.idle": "2024-05-08T19:37:15.320524Z",
     "shell.execute_reply": "2024-05-08T19:37:15.319248Z",
     "shell.execute_reply.started": "2024-05-08T19:36:04.368354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U datasets accelerate bitsandbytes trl peft evaluate git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T19:37:15.323385Z",
     "iopub.status.busy": "2024-05-08T19:37:15.322985Z",
     "iopub.status.idle": "2024-05-08T19:37:34.887136Z",
     "shell.execute_reply": "2024-05-08T19:37:34.885424Z",
     "shell.execute_reply.started": "2024-05-08T19:37:15.323353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 19:37:23.278274: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-08 19:37:23.278396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-08 19:37:23.423766: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import setup_chat_format, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peft\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=64, #128\n",
    "        lora_dropout=0.05,\n",
    "        r=8, #\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\"], #all_linear\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T19:37:42.771814Z",
     "iopub.status.busy": "2024-05-08T19:37:42.770807Z",
     "iopub.status.idle": "2024-05-08T19:37:42.778475Z",
     "shell.execute_reply": "2024-05-08T19:37:42.777080Z",
     "shell.execute_reply.started": "2024-05-08T19:37:42.771769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_quant_type', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"int8\",\n",
    "    bnb_8bit_compute_dtype=torch.int8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 8 bit not works 4 bit quantization\n",
    "'''quantization_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16 #but should be set to the optimal BFloat16 for newer hardware supporting it to achieve the best performance.\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Model ID\n",
    "model_id = \"Trendyol/Trendyol-LLM-7b-base-v1.0\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T19:38:12.406721Z",
     "iopub.status.busy": "2024-05-08T19:38:12.406296Z",
     "iopub.status.idle": "2024-05-08T19:38:16.634627Z",
     "shell.execute_reply": "2024-05-08T19:38:16.633592Z",
     "shell.execute_reply.started": "2024-05-08T19:38:12.406688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d82d4f2df4c4e189d38480625840668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a737b78f11423bb6dab6b3c881dd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/13293 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#will added our translated data\n",
    "dataset =  load_dataset(\"oguuzhansahin/chatdoctor-translated\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Sen hastalara yardım eden Sohbet Doktorusun. Hastaların şikayetlerini dinleyip onlara çözüm öner.\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "\n",
    "    return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": sample[\"input\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n",
    "    ]\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??\n",
    "#dataset = dataset.train_test_split(test_size = 0.02)\n",
    "dataset = dataset.map(create_conversation, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = \"your token\"\n",
    "login(token = hf_token,add_to_git_credential = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"selincildam/medical-chatbot-turkish\", # directory to save and repository id\n",
    "    num_train_epochs=5,                     # number of training epochs\n",
    "    per_device_train_batch_size=4,          # batch size per device during training\n",
    "    #gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    #gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=20,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    #bf16=True,                              # use bfloat16 precision\n",
    "    #tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    max_steps= 100\n",
    "#   report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "max_seq_length = 512 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "api_token = \"your api token\"\n",
    "model_name = \"medical-chatbot-turkish\"\n",
    "model.push_to_hub(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "### Load your finetuned model and generate model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ceydabasoglu/medical-chatbot-turkish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ceydabasoglu/medical-chatbot-turkish\")\n",
    "\n",
    "input_message = {'input' :'Sürekli baş ağrısı neden olur?',\n",
    "                 'output':'' }\n",
    "\n",
    "input_message = input_message.map(create_conversation)\n",
    "\n",
    "#inputs = tokenizer(\"\"\"###Human: Sürekli baş ağrısı neden olur? ###Assistant: \"\"\", return_tensors=\"pt\").to(\"cuda\") #write here your input\n",
    "\n",
    "inputs = tokenizer(input_message, return_tensors=\"pt\").to(\"cuda\") #write here your input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_name, #write model name\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_time = time.time()\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "resp_time = (time.time() - st_time) / 60\n",
    "print(response)\n",
    "print(f\"Response time:{resp_time} dk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"ceydabasoglu/medical-chatbot-turkish\"\n",
    "snapshot_download(repo_id = model_id, \n",
    "                  local_dir=\"myllama-hf\",\n",
    "                  local_dir_use_symlinks=False, \n",
    "                  revision=\"main\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''$ git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "$ pip install -r llama.cpp/requirements.txt\n",
    "\n",
    "$ python llama.cpp/convert.py myllama-hf \\\n",
    "  --outfile medical-chatbot-turkish-v0.1.gguf \\\n",
    "  --outtype q4_0''' #q8_0 olabilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "model_id = \"ceydabasoglu/medical-chatbot-turkish-v0.1.gguf\"\n",
    "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
    "api.upload_file(\n",
    "    repo_id=model_id,\n",
    "    path_or_fileobj=\"medical-chatbot-turkish-v0.1.gguf\",\n",
    "    path_in_repo=\"medical-chatbot-turkish-v0.1.gguf\"\n",
    ")'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
